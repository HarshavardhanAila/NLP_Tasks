{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wknRrNHwjodK"
   },
   "source": [
    "#**ASSIGNMENT 1**\n",
    "\n",
    "\n",
    "#**NAME: HARSHAVARDHAN AILA**\n",
    "\n",
    "\n",
    "#**UNT ID: 11637549**\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "---\n",
    "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
    "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Answer the questions in brief with examples.\n",
    "\n",
    "* Submit  \n",
    "1. The IPython Notebook (ipynb) file.  \n",
    "2. A PDF version of the notebook (converted from ipynb).\n",
    "\n",
    "* The similarity score should be less than 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0MGUnuLnXue"
   },
   "source": [
    "#**Task 1: Tokenization (25%)**\n",
    "(refer to the spacy tokenization concept which is explained after Question1 in activity-1)\n",
    "\n",
    "**Question -1:**\n",
    "\n",
    "##How can we incorporate contextual information beyond individual words into the tokenization process to improve performance on downstream tasks like machine translation or question answering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9jooJ_D1UQp"
   },
   "source": [
    "Answer Here: We need to consider the following tasks to incorporate the contextual information beyond individual words into the tokenization process to improve performance downline machine translation or question answering: Subword tokenization, wordpeice tokenization, sentence peice tokenization, Byte level or character level tokenization and task-specific tokenization.\n",
    "\n",
    "\n",
    "The Byte pair encoding or the unigram language model is used instead of dividing/splitting the text into individual words. The another subword tokenization approach is word piece whcih splits the words into smaller parts. It is same as BPE but differs in handling the spliiting process. It is widely used across NLP frameworks like GPT and BERT.\n",
    "\n",
    "\n",
    "Another Unsupoervised text tokenizerwhich can handle several tokenization tasks including the character and subword tokenization is SentencePiece which uses the unigran language model algorithm and can be trained on large corpus which captures contextual information effectively.\n",
    "\n",
    "The Byte-Level or Character-Level Tokenization can tokenize at the byte level or character level. This is useful when with languages that do not have a clear word boundaries or when dealing with where characters play a crucial role, example handwriting recognition.\n",
    "\n",
    "The Task-Specific Tokenization can design task-specific tokenization approaches which takes into account the specific requirements of the task. In the example of question answering, we may want to include special tokens to mark the start and end of the answer span.\n",
    "\n",
    "Finally, it's worth saying that tokenization is just the first step, and by using the pre-trained language models like BERT or GPT can further incorporate the contextual information during fine-tuning on downstream tasks. These models leverage transformer architectures that capture contextual dependencies between tokens through attention mechanisms, allowing them to understand the meaning of words in context.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIl7PgRaeGjn"
   },
   "source": [
    "#####**Question 2**\n",
    "##Develop a tokenizer that considers contractions like \"can't\" and \"doesn't\", splitting them into their constituent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aQ_rfINQfbA2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'not', 'believe', 'it', 'is', 'not', 'butter', '!', 'you', 'are', 'going', 'to', 'love', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "text=\"I can't believe it's not butter! You're going to love it.\"\n",
    "\n",
    "#CODE HERE\n",
    "import nltk\n",
    "\n",
    "contraction_mapping = {\n",
    "    \"can't\": \"can not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"you're\":\"you are\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "#here I am Tokenizing the text using a custom regular expression pattern:\n",
    "    pattern = r\"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
    "    tokens = nltk.regexp_tokenize(text, pattern)\n",
    "    \n",
    "#Here I am Expanding contractions:\n",
    "    expanded_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.lower() in contraction_mapping:\n",
    "            expanded_tokens.extend(contraction_mapping[token.lower()].split())\n",
    "        else:\n",
    "            expanded_tokens.append(token)\n",
    "    \n",
    "    return expanded_tokens\n",
    "#Given Text:\n",
    "text = \"I can't believe it's not butter! You're going to love it.\"\n",
    "expanded_tokens = expand_contractions(text)\n",
    "print(expanded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, I have imported the nltk library, then First using that I am trying to tokenize the custom regular expression patter. For example \"can't\" as short cut goes as cannot, Then I am trying to match the regular expression pattern with the contractions. In the next step i am tokenizing the text and then checking for tokens are contractions and then i am trying to expand and process . Then finally using the text it gives me the required constituent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BRekASOnmsz"
   },
   "source": [
    "\n",
    "**Question 3:**\n",
    "##Implement a Python script to remove Twitter username handles from a given twitter text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-MD-tQZKsCkL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great meeting with JohnDoe and JaneSmith today ! Looking forward to our next project . Thanks for the insights TechGuru ðŸš€ # innovation # teamwork\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def remove_username_handles(text):\n",
    "#Here I am Tokenizing the text into individual words:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "#Here I am Removing the tokens that start with '@' for the Twitter username handles:\n",
    "    cleaned_tokens = [token for token in tokens if not token.startswith('@')]\n",
    "    \n",
    "#Here I am Joining the cleaned tokens back into a single string:\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "#Given Text\n",
    "text = \"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru ðŸš€ #innovation #teamwork\"\n",
    "cleaned_text = remove_username_handles(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation: In the above task I have imported the nltk library and then performed tokenization for the text to split into individual words. Then I have removed the tokens which starts with '@' for the twitter username handles. Then I am joining the cleaned tokens into the single string to get back the updated sentence without the usernames. Then finally using the given text I have kept in the function and rettrived the required result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zME8pbf7KWZG"
   },
   "source": [
    "#**Task 2.  - Regular Expressions (25%)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRR6kMx1haKo"
   },
   "source": [
    "###**Regular Expressions**\n",
    "A regular expression, often abbreviated as regex, is a powerful and flexible tool for pattern matching and text manipulation. It consists of a sequence of characters that defines a search pattern, allowing you to perform various text-related tasks such as text validation, data extraction, text cleaning, and more. Regular expressions are used in programming languages and text editors and are constructed using a combination of regular characters, special characters, and metacharacters to specify search criteria. Learning to use regular expressions effectively can greatly enhance text processing tasks, making them a valuable skill in fields like natural language processing, data extraction, and data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhp7XiXXh-CQ"
   },
   "source": [
    "Python includes a builtin module called `re` which provides regular expression matching operations (Click [here](https://docs.python.org/3/library/re.html) for the official module documentation). Once the module is imported into your code, you can use all of the available capabilities for performing pattern-based matching or searching using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3L6v4jyYLh44"
   },
   "outputs": [],
   "source": [
    "##code block -1\n",
    "import re\n",
    "\n",
    "def apply_regex(data, pattern):\n",
    "  for text in data:\n",
    "    if re.fullmatch(pattern, text):\n",
    "      print(f\"Test string {text} accepted.\")\n",
    "    else:\n",
    "      print(f\"Test string {text} failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojyfJF7bymRl"
   },
   "source": [
    "\n",
    "\n",
    "##**Question - 1**\n",
    "\n",
    "##Same as previous question Implement a Python script to remove Twitter username handles from a given twitter text with Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wlcFDE341Idg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great meeting with  and  today! Looking forward to our next project. Thanks for the insights  ðŸš€ #innovation #teamwork\n"
     ]
    }
   ],
   "source": [
    "#CODE HERE\n",
    "import re\n",
    "\n",
    "def remove_twitterusername_handles(text):\n",
    "#Here I am using the Regular expression pattern to match the Twitter username handles:\n",
    "    twitterusername_handle_pattern = re.compile(r'@\\w+')\n",
    "    \n",
    "#Here I am Removing the username handles from the given text:\n",
    "    updated_text = re.sub(twitterusername_handle_pattern, '', text)\n",
    "    \n",
    "    return updated_text\n",
    "text=\"Great meeting with @JohnDoe and @JaneSmith today! Looking forward to our next project. Thanks for the insights @TechGuru ðŸš€ #innovation #teamwork\"\n",
    "updated_text = remove_twitterusername_handles(text)\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Explanation:In the above task, I have imported the regular expression and I have tried to implement a python script to remove twitter username handles by creating a function remove_twitterusername_handles and using the regular expressions pattern I tried to match the twitter username handles. In the next step, I have tried to remove username handles from the text. Now I ahve implemented the code using the given text and achieved the usernames less sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17YpjyzUo8oq"
   },
   "source": [
    "\n",
    "##**Question- 2**\n",
    "\n",
    "##Implement a Python program to find URLs in the given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gRsGKmmskWJT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls_traced: https://w3resource.com\n",
      "urls_traced: http://github.com\n",
      "urls_traced: https://openai.com\n",
      "urls_traced: https://docs.python.org\n"
     ]
    }
   ],
   "source": [
    "text= '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
    "##Your code here\n",
    "\n",
    "import re\n",
    "# Given Text:\n",
    "text = '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
    "\n",
    "#Here I am matching Regular expression pattern to URLs:\n",
    "pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "\n",
    "#Here I am Finding all URLs using the above pattern:\n",
    "url_traced = re.findall(pattern, text)\n",
    "\n",
    "# Printing the traced URLs:\n",
    "for urls in url_traced:\n",
    "    print(\"urls_traced:\",urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above Regular expression task, I have tried to import the regular expression as \"re\". Then using teh given text I have tried to match the regular expression using a pattern \"r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\". The expression is all about matching/tracing the URLs using r'https?:// which explains that URls start with either http:// or https://. This part \"s?:\" makes sure taht there is no space betweent he URLs.The \"|\" acts as the OR operator. This part \"www\\.[^\\s<>\"]+\" gives information about tracing the word that starts with \"www\" followed by not white spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf1Pm9CHs1LM"
   },
   "source": [
    "## **Using regular expressions based pattern matching on real world text**\n",
    "\n",
    "For the purposes of demonstration, here's a dummy paragraph of text. A few observations here:\n",
    "* The text has multiple paragraphs with each paragraph having more than one sentence.\n",
    "* Some of the words are capitalized (first letter is in uppercase followed by lowercase letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPLUtLxVtiTH",
    "outputId": "59ca3969-3381-4a70-fd3d-a42e97902fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
      "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
      "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
      "4th paragraph is not going to be detected by either of the regex patterns below.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
    "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
    "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
    "4th paragraph is not going to be detected by either of the regex patterns below.\n",
    "\"\"\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqmf8KSFtt2E"
   },
   "source": [
    "The following code block shows a regular expression that matches only those strings that:\n",
    "1. are at the start of a line and\n",
    "2. the string does not start with a number or a whitespace\n",
    "\n",
    "`re.findall()` finds all matches of the pattern in the text under consideration. The output is a list of strings that matched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyYWQ5Tzttzt"
   },
   "source": [
    "Further, the regular expression defined below matches the words that are capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxA3QOHHtqOg",
    "outputId": "0f566cde-274a-47a1-b953-67df8c4bd320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'First', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Now', 'Second', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence', 'Finally', 'Third', 'Paragraph', 'First', 'Sentence', 'Second', 'Sentence', 'Third', 'Sentence', 'Fourth', 'Sentence', 'Fifth', 'Sentence']\n"
     ]
    }
   ],
   "source": [
    "##code block - 3\n",
    "re_pattern2 = r'[A-Z][a-z]+'\n",
    "print(re.findall(re_pattern2, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0VUlW3RunCd"
   },
   "source": [
    "\n",
    "Following is a text excerpt on \"Inaugural Address\" taken from the website of the [Joint Congressional Committee on Inaugural Ceremonies](https://www.inaugural.senate.gov/inaugural-address/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wRxR7M-NuFtR"
   },
   "outputs": [],
   "source": [
    "inau_text=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inaugurationâ€”George Washingtonâ€™sâ€”on April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on recordâ€”just 135 words â€”before repeating the oath of office.\n",
    "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the Presidentâ€™s speech.\n",
    "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841â€”a bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adamsâ€™ Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washingtonâ€™s second Inaugural address, the next shortest was Franklin D. Rooseveltâ€™s fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nationâ€™s involvement in World War II.\n",
    "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidgeâ€™s Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
    "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, â€œWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nationâ€™s wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.â€ In 1933, Franklin D. Roosevelt avowed, â€œwe have nothing to fear but fear itself.â€ And in 1961, John F. Kennedy declared, â€œAnd so my fellow Americans: ask not what your country can do for youâ€”ask what you can do for your country.â€\n",
    "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jacksonâ€™s first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reaganâ€™s Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qMOPAWe3wnj"
   },
   "source": [
    "Refer to  above code block -3 for the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Oap3GNEut1G"
   },
   "source": [
    "\n",
    "##**Questions-3.A**\n",
    "## Identify all the positive and negative numbers with type of both intergers,float  in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of such words in the text. Then, run the Python code snippet to automatically display the matched strings according to the pattern.*.\n",
    "\n",
    "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BNqyUNbPurJm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number traced: 30\n",
      "Number traced: 1789\n",
      "Number traced: 04\n",
      "Number traced: -30\n",
      "Number traced: -1789\n",
      "Number traced: -18.5\n",
      "Number traced: 4\n",
      "Number traced: 1793\n",
      "Number traced: 03\n",
      "Number traced: 04\n",
      "Number traced: 1793\n",
      "Number traced: 135\n",
      "Number traced: 8\n",
      "Number traced: 445\n",
      "Number traced: 4\n",
      "Number traced: 1841\n",
      "Number traced: 2\n",
      "Number traced: 308\n",
      "Number traced: 737\n",
      "Number traced: 20\n",
      "Number traced: 1945\n",
      "Number traced: 01\n",
      "Number traced: -20\n",
      "Number traced: -1945\n",
      "Number traced: 559.0\n",
      "Number traced: 1921\n",
      "Number traced: 1925\n",
      "Number traced: 1949\n",
      "Number traced: 1865\n",
      "Number traced: 1933\n",
      "Number traced: 1961\n",
      "Number traced: 1829\n",
      "Number traced: 1829\n",
      "Number traced: 37.0\n",
      "Number traced: 1981\n",
      "Number traced: -55.5\n",
      "Number traced: 20.8\n",
      "Number traced: -3.0\n"
     ]
    }
   ],
   "source": [
    "##Your code here\n",
    "\n",
    "import re\n",
    "\n",
    "# Regular expression pattern to match positive and negative floating-point numbers\n",
    "re_pattern3 = r'-?\\d+(?:\\.\\d+)?'\n",
    "\n",
    "# Find all positive and negative floating-point numbers using the pattern\n",
    "numbers = re.findall(re_pattern3, inau_text)\n",
    "\n",
    "# Printing the matched numbers:\n",
    "for number in numbers:\n",
    "    print(\"Number traced:\",number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWQdvlG0dPKy"
   },
   "source": [
    "##Your explanation:\n",
    "\n",
    "In the above task, I have tried to implement a pattern which finds the positive and negative with both integer and float data type. For this I have imported regular expression using re. I have written a pattern to find the given as \"r'-?\\d+(?:\\.\\d+)?'\" where r is a raw string and the '-' indicates either positive or negative number. The 'd+' matches one or more digits. '?:\\.\\d+' matches a floating number with one or more digits. The .findall() function is used to return a list of the used pattern that occurs in given string.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mkbLF4nu7la"
   },
   "source": [
    "##**Question-3.B**\n",
    "##*Identify all the dates of all forms - text form(April 20, 1945) and digit form(xx-xx-xxxx, xx/xx/xxxx) in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of the dates in the text. Then, run the Python code snippet to automatically display a list of all such dates identified.*\n",
    "\n",
    "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2pxl72VhvEcG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April 30, 1789\n",
      "04-30-1789\n",
      "March 4, 1793\n",
      "03/04/1793\n",
      "March 4, 1841\n",
      "January 20, 1945\n",
      "01-20-1945\n"
     ]
    }
   ],
   "source": [
    "##Your code here\n",
    "import re\n",
    "\n",
    "#here i am giving a  Regular expression pattern to match dates in text form and digit form:\n",
    "date_pattern = r\"(?:\\b(?:\\w+\\s+\\d{1,2},\\s+\\d{4}|\\d{2}[/-]\\d{2}[/-]\\d{4})\\b)\"\n",
    "\n",
    "#Here i am Finding all occurrences of dates in the text using the pattern:\n",
    "dates = re.findall(date_pattern, inau_text)\n",
    "\n",
    "#here i am Printing the identified dates:\n",
    "for date in dates:\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdUT4vfidQ_L"
   },
   "source": [
    "##Your explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above task,we need to identify the dates of all forms i.e text form and digit form. I have built a date pattern where the '?:' is used to match two sub groups. The '\\b' represents a word boundary where the pattern matches at the start or end of a word. The '?:\\w+\\s+\\d{1,2},\\s+\\d{4}' subpattern matches the required \"Month Day, Year\" format. The another  '\\d{2}[/-]\\d{2}[/-]\\d{4}' subpattern matches the \"MM/DD/YYYY\" or \"MM-DD-YYYY\" format. In this way the pattern traces the date formats and displays us from given passage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZSO1IrN06xI"
   },
   "source": [
    "#**Task 3: Lemmatization/Stemming(25%)**\n",
    "\n",
    "\n",
    "#**Question -1:**\n",
    "##How does the morphology of a language (e.g., agglutinative vs. fusional) impact the suitability of stemming vs. lemmatization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80ycUYp41Iy_"
   },
   "source": [
    "##Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The morphology of a language, specifically whether it is agglutinative or fusional, can impact the suitability of stemming and lemmatization techniques for text processing. Understanding different types of morphological structures:\n",
    "\n",
    "1. Stemming: It is a process of reducing words to their base or root form, called a stem which involves removing prefixes, suffixes, and inflectional endings from words. Relatively regular and transparent word structureAgglutinative languages are well-suited for stemming because of the morphological components in the agglutinative languages are often clearly separable, making it easier to identify and remove affixes.Stemming is particularly effective in agglutinative languages because the affixes are typically easily identifiable and separable.\n",
    "\n",
    "2. Lemmatization: It involves determining the base or dictionary form of a word, known as the lemma.  The word's context and grammatical features to produce the correct base form. Fusional languages, which often have complex and irregular word forms, benefit more from lemmatization. This is because lemmatization considers the syntactic and semantic information of a word to produce its base form, which can vary significantly in fusional languages.Lemmatization is more suitable for fusional languages because it considers the word's context, syntactic role, and grammatical features to determine the base or dictionary form of a word.\n",
    "\n",
    "In conclusion, the choice between stemming and lemmatization depends on the morphology of the language being processed. Agglutinative languages with regular and transparent structures are more suitable for stemming, while fusional languages with complex and irregular word forms benefit from lemmatization. In practical text processing applications, it's often beneficial to consider both stemming and lemmatization approaches, depending on the requirements and linguistic characteristics of the language being analyzed. However, it's important to note that the effectiveness of these techniques can vary depending on the specific language and the context in which they are applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMMf51DdsaK5"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    " ## **Question - 2 :**\n",
    "\n",
    "\n",
    "## Create a Python function that takes a sentence as input, performs lemmatization using **StanfordNLP**, and removes stopwords from the lemmatized sentence. Use a list of stopwords. Return the cleaned and lemmatized sentence.\n",
    "\n",
    "Reference:https://stanfordnlp.github.io/stanfordnlp/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanfordnlp in c:\\users\\19408\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanfordnlp) (2.28.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanfordnlp) (4.25.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanfordnlp) (4.64.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanfordnlp) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanfordnlp) (2.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.0.0->stanfordnlp) (2024.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanfordnlp) (2.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\19408\\anaconda3\\lib\\site-packages (from tqdm->stanfordnlp) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.0.0->stanfordnlp) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from sympy->torch>=1.0.0->stanfordnlp) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.8.0-py3-none-any.whl (970 kB)\n",
      "     -------------------------------------- 970.4/970.4 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (1.21.5)\n",
      "Requirement already satisfied: networkx in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (2.8.4)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (2.1.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (4.25.3)\n",
      "Requirement already satisfied: toml in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (4.64.1)\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
      "     ------------------------------------- 421.5/421.5 kB 27.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\19408\\anaconda3\\lib\\site-packages (from stanza) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\19408\\anaconda3\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanza) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanza) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanza) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->stanza) (1.26.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\19408\\anaconda3\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.2.1)\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.10.1 stanza-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1137a87ac1eb4eaeb0b46c6024b5754c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 14:20:51 INFO: Downloaded file to C:\\Users\\19408\\stanza_resources\\resources.json\n",
      "2024-02-29 14:20:51 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-02-29 14:20:53 INFO: File exists: C:\\Users\\19408\\stanza_resources\\en\\default.zip\n",
      "2024-02-29 14:20:58 INFO: Finished downloading models and saved to C:\\Users\\19408\\stanza_resources\n",
      "2024-02-29 14:20:58 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187a5b0ce7f84fd3a5de66929e22c23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 14:20:59 INFO: Downloaded file to C:\\Users\\19408\\stanza_resources\\resources.json\n",
      "2024-02-29 14:21:01 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-02-29 14:21:01 INFO: Using device: cpu\n",
      "2024-02-29 14:21:01 INFO: Loading: tokenize\n",
      "2024-02-29 14:21:01 INFO: Loading: mwt\n",
      "2024-02-29 14:21:01 INFO: Loading: pos\n",
      "2024-02-29 14:21:02 INFO: Loading: lemma\n",
      "2024-02-29 14:21:02 INFO: Loading: constituency\n",
      "2024-02-29 14:21:02 INFO: Loading: depparse\n",
      "2024-02-29 14:21:03 INFO: Loading: sentiment\n",
      "2024-02-29 14:21:03 INFO: Loading: ner\n",
      "2024-02-29 14:21:04 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict future magic , artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "#CODE HERE\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_and_lemmatize_sentence(sentence):\n",
    "    stanza.download('en')\n",
    "    \n",
    "#Here i have Loaded the English model for stanza:\n",
    "    nlp = stanza.Pipeline('en')\n",
    "    \n",
    "# Here I Lemmatized the sentence using stanza:\n",
    "    doc = nlp(sentence)\n",
    "    lemmas = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemmas.append(word.lemma)\n",
    "    \n",
    "#Here I Removed the stopwords from the lemmatized sentence:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_lemmas = [lemma for lemma in lemmas if lemma.lower() not in stop_words]\n",
    "    \n",
    "#Here I  Returned the cleaned and lemmatized sentence:\n",
    "    cleaned_sentence = ' '.join(cleaned_lemmas)\n",
    "    return cleaned_sentence\n",
    "sentence = \"The predicting of future is'nt magic, Its Artificial Intelligence\"\n",
    "cleaned_lemmatized_sentence = clean_and_lemmatize_sentence(sentence)\n",
    "print(cleaned_lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri0OIiWfBN8R"
   },
   "source": [
    "##Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above task, I have tried to import stanfordnlp but given a note that All development, issues, ongoing maintenance, and support have been moved to new GitHub repository as the toolkit is being renamed as Stanza since version 1.0.0. It was given to visit new website for more information where we can still download stanfordnlp via pip, but newer versions of this package will be made available as stanza. I have used  stanza for lemmatization, stopwords from nltk.corpus for stopwords, and word_tokenize from nltk.tokenize for tokenization Using NLTK library it provides a wide range of natural language processing functionalities and created a lemmatize_and_remove_stopwords function for removal of stop words. In this I have achieved lemmatization of sentence using different stopwords. The final words are printed as output which has no stopwords in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7o1E0wTupIA"
   },
   "source": [
    "##**Question - 3**\n",
    "\n",
    "(refer to the byte pair encoding concept which is explained in activity-2 at the Tutorial of Subword Tokenization using HuggingFace after the Question6 in activity-2 )\n",
    "\n",
    "\n",
    "Consider the following two sentences:\n",
    "\n",
    "**S1:**I like yellow roses better than red ones.\n",
    "\n",
    "**S2:**Looks like John is bettering the working conditions at his organization\n",
    "\n",
    "**Create a Python function that encodes two sentences using the custom BPE tokenizer and identifies common subword tokens (tokens that appear in both encodings). Return a list of these common subword tokens. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3RY65-RPvURG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ä ', 's', 'a', 'm', 'Ä i', 'te', 'r', 'i', 'Ä subject', 'f', 'o', 'n', 'l']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def encode_and_find_common_subwords(sentence1, sentence2):\n",
    "#Here I have Created temporary text files:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as temp_file1, tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as temp_file2:\n",
    "        temp_file1.write(sentence1)\n",
    "        temp_file2.write(sentence2)\n",
    "        temp_file1.flush()\n",
    "        temp_file2.flush()\n",
    "        temp_file_paths = [temp_file1.name, temp_file2.name]\n",
    "\n",
    "#here I have Initialized a ByteLevelBPETokenizer:\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "#Here I have Trained the tokenizer on the temporary files:\n",
    "        tokenizer.train(files=temp_file_paths, vocab_size=1000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# This is for Encoded sentence 1:\n",
    "        encoded_sentence1 = tokenizer.encode(sentence1).tokens\n",
    "\n",
    "#This is for Encode sentence 2:\n",
    "        encoded_sentence2 = tokenizer.encode(sentence2).tokens\n",
    "\n",
    "#This is to Find common subword tokens:\n",
    "        common_tokens = list(set(encoded_sentence1) & set(encoded_sentence2))\n",
    "#To Remove temporary files:\n",
    "    for temp_file_path in temp_file_paths:\n",
    "        os.remove(temp_file_path)\n",
    "\n",
    "    return common_tokens\n",
    "sentence1 = \"nlp is my favourite subject \"\n",
    "sentence2 = \"most of the subjects I like are related to artificial intelligence\"\n",
    "\n",
    "common_tokens = encode_and_find_common_subwords(sentence1, sentence2)\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exlpanation for above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above task, I have imported the 'ByteLevelBPETokenizer' from the tokenizer and also I have imported the tempfile and os. I have used sentence-1 and sentence-2 which takes as inputs for the encode_and_find_common_subwords function.The temporary files are pushed into contents to write on disk.The `tokenizer.encode(sentence1).tokens` encodes `sentence1` and `tokenizer.encode(sentence2).tokens` encodes `sentence2` using the trained tokenizer and returns a list of subword tokens. \n",
    "The `encode_and_find_common_subwords` function by providing two example sentences: `sentence1` and `sentence2`. I have printed the output, which shows the list of common subword tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEKqJAYbvUG3"
   },
   "source": [
    "# **Task - 4 : Minimum Edit distance (25%)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##Minimum edit Distance\n",
    "Minimum Edit Distance (also known as Levenshtein Distance) is a measure of similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into the other. It has applications in various fields, including natural language processing, spell checking, DNA sequence alignment, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNZzSXu2xoAI"
   },
   "source": [
    "# Character Based Text Similarity\n",
    "\"As an example, this technology is used by information retrieval systems, search engines, automatic indexing systems, text summarizers, categorization systems, plagiarism checkers, speech recognition, rating systems, DNA analysis, and profiling algorithms (IR/AI programs to automatically link data between people and what they do).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHA_ccV_xmCB",
    "outputId": "0f8944cb-413d-4740-dd8e-5ba112fbb6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "##code block -4\n",
    "# A Naive recursive Python program to find minimum number\n",
    "# operations to convert str1 to str2\n",
    "\n",
    "\n",
    "def editDistance(str1, str2, m, n):\n",
    "\n",
    "    # If first string is empty, the only option is to\n",
    "    # insert all characters of second string into first\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    # If second string is empty, the only option is to\n",
    "    # remove all characters of first string\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    # If last characters of two strings are same, nothing\n",
    "    # much to do. Ignore last characters and get count for\n",
    "    # remaining strings.\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "        return editDistance(str1, str2, m-1, n-1)\n",
    "\n",
    "    # If last characters are not same, consider all three\n",
    "    # operations on last character of first string, recursively\n",
    "    # compute minimum cost for all three operations and take\n",
    "    # minimum of three values.\n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
    "                   editDistance(str1, str2, m-1, n),    # Remove\n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
    "                   )\n",
    "\n",
    "\n",
    "# Driver code\n",
    "str1 = \"sunday\"\n",
    "str2 = \"saturday\"\n",
    "print (editDistance(str1, str2, len(str1), len(str2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSfpOff4Ngbx"
   },
   "source": [
    "Refer above code block -4  for the following question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUJ1n35-dgON"
   },
   "source": [
    "##Question-1\n",
    "##Assuming case sensitivity where changing a letter's case has a cost of 1, calculate the minimum cost to transform \"Imagine\" into \"imagination\" with the following operation costs: insertions = 2, deletions = 2, substitutions = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bt4ZjDL2c3LB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "##your code here\n",
    "\n",
    "def editDistance(str1, str2, m, n, case_insensitive=False):\n",
    "    if m == 0:\n",
    "        return n\n",
    "\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if case_insensitive and str1[m-1].lower() == str2[n-1].lower():\n",
    "        return editDistance(str1, str2, m-1, n-1, case_insensitive)\n",
    "\n",
    "    if str1[m-1] == str2[n-1]:\n",
    "        return editDistance(str1, str2, m-1, n-1, case_insensitive)\n",
    "\n",
    "    if case_insensitive:\n",
    "        cost = 1\n",
    "    else:\n",
    "        cost = 3\n",
    "\n",
    "    return min(\n",
    "        #This is used for Insertion:\n",
    "        cost + editDistance(str1, str2, m, n-1, case_insensitive), \n",
    "        #This is used for Removal:\n",
    "        cost + editDistance(str1, str2, m-1, n, case_insensitive),\n",
    "        #This is used for replacing:\n",
    "        cost + editDistance(str1, str2, m-1, n-1, case_insensitive)  \n",
    "    )\n",
    "\n",
    "\n",
    "str1 = \"Imagine\"\n",
    "str2 = \"imagination\"\n",
    "case_insensitive = True\n",
    "print(editDistance(str1, str2, len(str1), len(str2), case_insensitive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fdCkNgZdL7_"
   },
   "source": [
    "##Your explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above task I have used edit distance function, in that string-1 and string-2 are two inputs and m,n are lengths of the strings. case-insensitive function gives whether the word is sensitive or not. If length of one string is 0 it returns to another string. The edit distance continues checking for low cost distance. The transformation happens between the strings imagine and imagination and it gives the final cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFiLMn7rIHUd"
   },
   "source": [
    "##Tutorial -2\n",
    "# Levenshtein Distance for Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FypLc-dkO96G",
    "outputId": "2a43ae34-99b2-4de4-e2d6-2bae18be35d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance between 'This is a cat' and 'That is a dog' with substitution cost 2 is 10\n"
     ]
    }
   ],
   "source": [
    "#code block - 5\n",
    "# Simple Minimum Edit Distance\n",
    "def levenshtein_distance(str1, str2):\n",
    "    # Initialize a matrix to store edit distances\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Initialize the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    # Fill in the matrix using dynamic programming\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if str1[i - 1] == str2[j - 1] else 2  # Substitution cost is 2\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,  # Deletion\n",
    "                dp[i][j - 1] + 1,  # Insertion\n",
    "                dp[i - 1][j - 1] + cost,  # Substitution\n",
    "            )\n",
    "\n",
    "    # The final value in the matrix represents the Levenshtein distance\n",
    "    return dp[m][n]\n",
    "\n",
    "# Example usage\n",
    "str1 = \"This is a cat\"\n",
    "str2 = \"That is a dog\"\n",
    "distance = levenshtein_distance(str1, str2)\n",
    "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with substitution cost 2 is {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-xrzKVC5iSe"
   },
   "source": [
    "Refer to above code block -5 from tutorial - 2 for the following question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EapfLAq5iCLo"
   },
   "source": [
    "##Question:2\n",
    "##Assign different costs to insertions, deletions, and substitutions to reflect varying penalties for different types of edits. Calculate the Levenshtein distance with these weighted costs.\n",
    "\n",
    "String1 = (\"Natural language processing\")\n",
    "\n",
    "String2 = (\"Computer science department\")\n",
    "\n",
    "Provide your explanation in the tex block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kcDvlYTD4YvR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance between 'Natural language processing' and 'Computer science department' with weighted costs is 46\n"
     ]
    }
   ],
   "source": [
    "##ENTER YOUR CODE HERE\n",
    "def levenshtein_distance(str1, str2, deletion_cost, insertion_cost, substitution_cost):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i * deletion_cost\n",
    "\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j * insertion_cost\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if str1[i - 1] == str2[j - 1] else substitution_cost\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + deletion_cost,\n",
    "                dp[i][j - 1] + insertion_cost,\n",
    "                dp[i - 1][j - 1] + cost,\n",
    "            )\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "str1 = \"Natural language processing\"\n",
    "str2 = \"Computer science department\"\n",
    "deletion_cost = 1\n",
    "insertion_cost = 3\n",
    "substitution_cost = 2\n",
    "\n",
    "distance = levenshtein_distance(str1, str2, deletion_cost, insertion_cost, substitution_cost)\n",
    "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with weighted costs is {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "texg77AFdG6A"
   },
   "source": [
    "##Your explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, I have modified and added 3 parameter to the provided code which represents deletions, insertions and substitutions. I have set deletion_cost to 1 substitution_cost to 2 and insertion_cost to 3 for the different types of edits. The Levenshtein distance between the two strings with the weighted costs is then calculated using the modified levenshtein_distance function, and the result is printed to the console. The output for the Levenshtein distance between \"Natural language processing\" and \"Computer science department\" considering the specified weighted costs for different types of edits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLV2Db4O0pj8"
   },
   "source": [
    "##Question-3\n",
    "##Describe how MED is used in various NLP tasks, such as spell checking, speech recognition, text summarization, and machine translation. How does its effectiveness vary across these tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfyAF_SacJZd"
   },
   "source": [
    "##Your Explanation Here\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Minimum edit distance(MED) is used in various NLP tasks in finding the similarity and dissimilarity in between two strings. \n",
    "In spell checking is used to identify and correct the misspelled words. It also calculates the MED between misspelled and correct one. The speech recognition in context of MED is used for translation of word or phrase. comparing features of input helps to identify closest map. When it comes to text summarization MED can be used for finding the similarities among sentences or docs. Machine translation used MED for translation of words. It gives between source and targeted values. On conclusion the effectiveness of MED not only depend on NLP tasks but also on different characteristics and complexties of the given task. It aslo depends on quality of training, availability, algorthms required, advance techniques and challenges to each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
