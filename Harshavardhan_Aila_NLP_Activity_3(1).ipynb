{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur1Spl3neyQc"
   },
   "source": [
    "# **Activity 3: Normalization**\n",
    "**Instructions:**\n",
    "\n",
    "---\n",
    "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
    "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Conclude your activity with a comprehensive explanation of your overall approach, aiming for about 200 words, in the final section of the notebook.\n",
    "\n",
    "* Submit  \n",
    "1. The IPython Notebook (ipynb) file.  \n",
    "2. A PDF version of the notebook (converted from ipynb).\n",
    "\n",
    "* The similarity score should be less than 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harshavardhan_Aila_ UNT ID:11636549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8hFNQohPORn"
   },
   "source": [
    "# **Text Preprocessing Beyond Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZIYqdWPmHr_"
   },
   "source": [
    "##**Word** **normalization**\n",
    "**Word normalization** is a text preprocessing technique used in natural language processing (NLP) to standardize and simplify words or tokens in a text document. The goal of word normalization is to make text data more consistent and manageable for analysis. This process can involve various transformations, such as converting all text to lowercase, removing punctuation, expanding contractions, and performing tasks like stemming or lemmatization to reduce words to their base or dictionary forms. Word normalization helps improve the accuracy and effectiveness of NLP tasks by reducing the complexity of text data and ensuring that similar words are treated as equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peiyYN47rMy1",
    "outputId": "5b6ebed5-6dd3-4e61-b8a8-20ac33c65aae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\19408\\anaconda3\\lib\\site-packages (4.38.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\19408\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# for using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for using SpaCy\n",
    "import spacy\n",
    "\n",
    "# for HuggingFace\n",
    "!pip install transformers\n",
    "# !pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y3Bh5T8sJXFQ"
   },
   "outputs": [],
   "source": [
    "# trick to wrap text to the viewing window for this notebook\n",
    "# Ref: https://stackoverflow.com/questions/58890109/line-wrapping-in-collaboratory-google-results\n",
    "#helps improve the readability and formatting of code output in the notebook.\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id3muvWNlNj6"
   },
   "source": [
    "### **Revisiting Tokenization** :**TreebankWordTokenizer**\n",
    "\n",
    "The **Treebank Word Tokenizer** is a text processing tool used in natural language processing (NLP) to split text into individual words or tokens. It follows the tokenization conventions and standards of the Penn Treebank corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "KiRKo3DKlfuQ",
    "outputId": "ec859ab3-4ed5-4729-f2d6-d9e511ae0d83"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Hello', 'everyone.', 'Welcome', 'to', 'NLP', 'Course', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "text=\"Hello everyone. Welcome to NLP Course.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43OOaeg7o78C"
   },
   "source": [
    "##**Using Regular Expression**\n",
    "\n",
    "**RegexpTokenizer** is a text processing tool provided by the Natural Language Toolkit (NLTK) library in Python. It is used to tokenize (split) text into individual tokens (words or phrases) based on a specified regular expression pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "me379siro8Jp",
    "outputId": "e350f297-f10c-4580-dbfa-3b18165862c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[\"Let's\",\n",
       " 'see',\n",
       " 'how',\n",
       " \"it's\",\n",
       " 'working',\n",
       " 'We',\n",
       " 'also',\n",
       " 'have',\n",
       " 'digits',\n",
       " 'like',\n",
       " '123',\n",
       " 'and',\n",
       " '010']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")#[\\w']+ as a whole matches sequences of word characters (letters, digits, underscores) and single quotes in a string\n",
    "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaHfuJo-phTQ"
   },
   "source": [
    "**Question 1:** Copy the above snippet code and Modify the above regular expression to match only **digits** from the above given text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ZiQuSrzQphoo",
    "outputId": "d6fcd2c1-0b63-4360-9b69-6a00a326a725"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['123', '010']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CODE HERE\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\d+')\n",
    "text = \"Let's see how it's working. We also have digits like 123 and 010\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgaX-Ck7YYzY"
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNUmlAHIQ9gs"
   },
   "source": [
    "## **(Tutorial) Stemming and Lemmatization using NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxa-khLuaEZJ"
   },
   "source": [
    "**Stemming** is a text normalization technique in natural language processing and information retrieval. It involves reducing words to their root or base form, often by removing suffixes or prefixes. The goal of stemming is to convert words with the same meaning but different forms into a common base form so that they can be treated as equivalent during text analysis and retrieval. Stemming helps improve information retrieval and text processing tasks by reducing the complexity of words while maintaining their core meaning. Common stemming algorithms include the Porter Stemmer and Snowball Stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TPOS73taHEq"
   },
   "source": [
    "##**Porter Stemmer**\n",
    "The **Porter Stemmer** is a well-known algorithm for stemming in natural language processing. It was  designed to reduce words to their root or base form by removing common suffixes. Stemming is the process of reducing words to their linguistic root or base form to simplify text analysis and improve information retrieval.For example, it can convert words like \"running,\" \"runs,\" and \"ran\" to their common root \"run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jCebsYwiQxf"
   },
   "source": [
    "Let's see how we can perform stemming and lemmatization using NLTK library..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3M0mDUIiSdu",
    "outputId": "bfb8a162-d6c8-4522-cc42-c7151c46fc40"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cats' after stemming: cat\n",
      "'better' after stemming: better\n",
      "'abaci' after stemming: abaci\n",
      "'aardwolves' after stemming: aardwolv\n",
      "'generically' after stemming: gener\n"
     ]
    }
   ],
   "source": [
    "# importing PorterStemmer class from nltk.stem module\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()    # instantiating an object of the PorterStemmer class\n",
    "\n",
    "stem = porter.stem('cats')    # calling the stemmer algorithm on the desired word\n",
    "print(f\"'cats' after stemming: {stem}\")\n",
    "\n",
    "stem = porter.stem('better')\n",
    "print(f\"'better' after stemming: {stem}\")\n",
    "\n",
    "stem = porter.stem('abaci')\n",
    "print(f\"'abaci' after stemming: {stem}\")\n",
    "\n",
    "stem = porter.stem('aardwolves')\n",
    "print(f\"'aardwolves' after stemming: {stem}\")\n",
    "\n",
    "stem = porter.stem('generically')\n",
    "print(f\"'generically' after stemming: {stem}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOhnG5IIcaHR"
   },
   "source": [
    "##**Lemmatization**\n",
    "**Lemmatization** is a natural language processing technique that reduces words to their base or dictionary form, known as a \"lemma.\" Unlike stemming, which often involves removing suffixes to approximate a word's root, lemmatization considers the word's context and grammatical meaning. The goal is to transform different inflected forms of a word into a common base form. Lemmatization is particularly useful for maintaining the grammatical correctness of words in text analysis and information retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tURX72sldJvF"
   },
   "source": [
    "##**WordNet Lemmatizer**\n",
    "The **WordNet Lemmatizer** is a lemmatization tool based on WordNet, a lexical database of the English language. WordNet groups words into sets of synonyms called \"synsets\" and provides a rich lexical and semantic structure for the English language. The WordNet Lemmatizer uses this semantic information to perform lemmatization, which is the process of reducing words to their base or dictionary form (lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wVrYUPXS2Oy",
    "outputId": "b8d9184b-ec41-4690-a238-d4cf88b70715"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Jo-qgQfjuli",
    "outputId": "ae38960a-e85e-4225-bd0d-31e7d7ba129a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cats' after lemmatization: cat\n",
      "'better' after lemmatization: better\n",
      "'abaci' after lemmatization: abacus\n",
      "'aardwolves' after lemmatization: aardwolf\n",
      "'generically' after lemmatization: generically\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'better' (as an adjective) after lemmatization: good\n"
     ]
    }
   ],
   "source": [
    "# importing WordNet-based lemmatizer class from nltk.stem module\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()    # instantiating an object of the WordNetLemmatizer class\n",
    "\n",
    "lemma = lemmatizer.lemmatize('cats')    # calling the lemmatization algorithm on the desired word\n",
    "print(f\"'cats' after lemmatization: {lemma}\")\n",
    "\n",
    "lemma = lemmatizer.lemmatize('better')\n",
    "print(f\"'better' after lemmatization: {lemma}\")\n",
    "\n",
    "lemma = lemmatizer.lemmatize('abaci')\n",
    "print(f\"'abaci' after lemmatization: {lemma}\")\n",
    "\n",
    "lemma = lemmatizer.lemmatize('aardwolves')\n",
    "print(f\"'aardwolves' after lemmatization: {lemma}\")\n",
    "\n",
    "lemma = lemmatizer.lemmatize('generically')\n",
    "print(f\"'generically' after lemmatization: {lemma}\")\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "lemma = lemmatizer.lemmatize('better', pos='a')   # 'a' denoted ADJECTIVE part-of-speech\n",
    "print(f\"'better' (as an adjective) after lemmatization: {lemma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDGVeoHqrQkF"
   },
   "source": [
    "### **Stemming on text string**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0OILkAMtu6iS",
    "outputId": "db93233c-76e0-489c-f163-bac9d3095af4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is jump , and he jump over the jump .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_text(text):\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Apply the stemmer to each word and join them back into a text\n",
    "    stemmed_text = ' '.join([stemmer.stem(word) for word in words])\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "# Example usage:\n",
    "text = \"He is jumping, and he jumped over the jumps.\"\n",
    "stemmed_text = stem_text(text)\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "TCtK0QouYj2d",
    "outputId": "9aa6408c-da12-4a37-c033-b10d3337dd7e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given text:\n",
      "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n"
     ]
    }
   ],
   "source": [
    "# This is the text on which you have to perform stemming; taken from Wikipedia.\n",
    "text = \"In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form; generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\"\n",
    "print(\"Given text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl9AIuruwUrE"
   },
   "source": [
    "**Tutorial 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "el7w7c7HmY9a",
    "outputId": "45f05b16-d04c-4680-ec83-10628001e98c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After punctuation removal:\n",
      "in linguistic morphology and information retrieval stemming is the process of reducing inflected or sometimes derived words to their word stem base or root form generally a written word form the stem need not be identical to the morphological root of the word it is usually sufficient that related words map to the same stem even if this stem is not in itself a valid root\n",
      "\n",
      "\n",
      "After stopword removal:\n",
      "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'words', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'words', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
     ]
    }
   ],
   "source": [
    "#CODE BLOCK 1\n",
    "en_stopwords = set(stopwords.words('english'))\n",
    "def remove_punc(text_string):\n",
    "  return re.sub('[^a-zA-Z0-9 ]', '', text_string.lower())\n",
    "\n",
    "def remove_stopwords(text_string):\n",
    "  return [ token for token in text_string.split(' ') if token not in en_stopwords ]\n",
    "\n",
    "# applying punctuation removal to the text\n",
    "unpunc_text = remove_punc(text)\n",
    "print(\"After punctuation removal:\")\n",
    "print(unpunc_text)\n",
    "\n",
    "# # applying stopword removal to the text\n",
    "clean_text = remove_stopwords(unpunc_text)\n",
    "print(\"\\n\\nAfter stopword removal:\")\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac5uju7eGVfg"
   },
   "source": [
    "#### **Question 2. Perform stemming on the cleaned text(from tutorial2-code block 1) above using the Porter Stemmer from NLTK.**\n",
    "\n",
    "**Hint:** import PorterStemmer from nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "jAC8FFLCErdI",
    "outputId": "d0a17ef5-fc94-47cc-a470-f5ccd7390da2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After stemming:\n",
      "['linguist', 'morpholog', 'inform', 'retriev', 'stem', 'process', 'reduc', 'inflect', 'sometim', 'deriv', 'word', 'word', 'stem', 'base', 'root', 'form', 'gener', 'written', 'word', 'form', 'stem', 'need', 'ident', 'morpholog', 'root', 'word', 'usual', 'suffici', 'relat', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
     ]
    }
   ],
   "source": [
    "# apply Porter Stemmer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
    "#CODE HERE\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_text = [stemmer.stem(token) for token in clean_text]\n",
    "\n",
    "print(\"\\n\\nAfter stemming:\")\n",
    "print(stemmed_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bUoRnNN6t0F"
   },
   "source": [
    "# **Lemmatization on text string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ib7DcVk3r9Aj",
    "outputId": "6f8797a6-4318-4989-a82e-e8451e192067"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\19408\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fp2nzJba6sJZ",
    "outputId": "66d25c75-be1d-4f08-ef55-eef7c45e66ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: There are like more than 100 foxes and lions in this forest.\n",
      "Lemmatized Text: There are like more than 100 fox and lion in this forest.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example text to lemmatize\n",
    "text = \"There are like more than 100 foxes and lions in this forest.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = text.split()\n",
    "\n",
    "# Lemmatize each word and join them back into a sentence\n",
    "lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# Print the original and lemmatized text\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N44TNDasS6eK"
   },
   "source": [
    "#### **Question 3. Perform lemmatization on the same cleaned text(from tutorial2-code block 1) above using NLTK's lemmatizer.**\n",
    "\n",
    "**Hint**:import WordNetLemmatizer from nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "tJ-moA25Erh3",
    "outputId": "82ed775f-9d33-447e-a47b-451a1e6b0f20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After lemmatization:\n",
      "['linguistic', 'morphology', 'information', 'retrieval', 'stemming', 'process', 'reducing', 'inflected', 'sometimes', 'derived', 'word', 'word', 'stem', 'base', 'root', 'form', 'generally', 'written', 'word', 'form', 'stem', 'need', 'identical', 'morphological', 'root', 'word', 'usually', 'sufficient', 'related', 'word', 'map', 'stem', 'even', 'stem', 'valid', 'root']\n"
     ]
    }
   ],
   "source": [
    "# apply NLTK's lemmatizer on the cleaned text (after punctuation and stopwords are removed) below this comment\n",
    "#CODE HERE\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_text = [lemmatizer.lemmatize(token, pos='n') for token in clean_text]\n",
    "\n",
    "print(\"\\n\\nAfter lemmatization:\")\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJNe9XfmOcqi"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYE8A-Mk4BKe"
   },
   "source": [
    "## **(Tutorial) Subword Tokenization using HuggingFace**\n",
    "\n",
    "**Hugging Face** is used for subword tokenization by offering NLP practitioners access to pre-trained subword tokenizers and models. Hugging Face's \"transformers\" library offers pre-trained models and tokenizers, such as Byte Pair Encoding (BPE) and SentencePiece, which are widely used for subword tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y48GHrsfgHI5"
   },
   "source": [
    "**Subword tokenization** is a text processing technique used in natural language processing (NLP) to break down words into smaller units, often subword pieces. This approach is particularly useful for handling languages with complex morphology or when dealing with out-of-vocabulary words. Subword tokenization methods like Byte-Pair Encoding (BPE) and SentencePiece divide text into subword units, such as character-level tokens or subword pieces, allowing NLP models to work with a more extensive and adaptable vocabulary. This technique improves the handling of rare words and enhances the performance of NLP models on a wide range of languages and tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\19408\\anaconda3\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from tokenizers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\19408\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\19408\\anaconda3\\lib\\site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-medium-vocab.json', <http.client.HTTPMessage at 0x17d0bc55b50>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\"\n",
    "filename = \"gpt2-medium-vocab.json\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('gpt2-merges.txt', <http.client.HTTPMessage at 0x17d1bc36130>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url2 = \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\"\n",
    "filename2 = \"gpt2-merges.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url2, filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bV2KUGEhIm2M",
    "outputId": "644a1aeb-0295-42ca-d2d3-a5f0da16e8b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers\n",
    "#This is a JSON file that contains the vocabulary (i.e., the set of words and subword pieces) used by the GPT-2 model\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wdDDf6VgnwQ"
   },
   "source": [
    "**Byte Pair Encoding (BPE)** is a subword tokenization technique used in natural language processing (NLP) and text processing. It involves dividing text into subword units, typically based on frequency, to create a more flexible and adaptive vocabulary for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cyiWXf-hLtRF",
    "outputId": "25867833-de29-4eb3-c4d3-8cb9184b6a4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[464, 2183, 286, 13630, 281, 2209, 319, 554, 7493, 3924, 3596, 2067, 351, 262, 845, 717, 554, 7493, 3924, 960, 20191, 2669, 447, 247, 82, 960, 261, 3035, 1542, 11, 1596, 4531, 13]\n",
      "['The', 'Ġcustom', 'Ġof', 'Ġdelivering', 'Ġan', 'Ġaddress', 'Ġon', 'ĠIn', 'aug', 'uration', 'ĠDay', 'Ġstarted', 'Ġwith', 'Ġthe', 'Ġvery', 'Ġfirst', 'ĠIn', 'aug', 'uration', 'âĢĶ', 'George', 'ĠWashington', 'âĢ', 'Ļ', 's', 'âĢĶ', 'on', 'ĠApril', 'Ġ30', ',', 'Ġ17', '89', '.']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "gpt2vocab = \"gpt2-medium-vocab.json\"\n",
    "gpt2merges = \"gpt2-merges.txt\"\n",
    "\n",
    "bpe = ByteLevelBPETokenizer(gpt2vocab, gpt2merges)\n",
    "bpe_encoding = bpe.encode(\"The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\")\n",
    "print(bpe_encoding.ids)\n",
    "print(bpe_encoding.tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvqxvTMqj5qh"
   },
   "source": [
    "**Question 4**: Collect the encoding ids which you generate from above snippet and now decode the ids to get back the given text string?\n",
    "\n",
    "**Hint**: use decode() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GaRsld66j5Ih",
    "outputId": "3fdfcddd-2025-49e9-b2b5-0ecc07345d0c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The custom of delivering an address on Inauguration Day started with the very first Inauguration—George Washington’s—on April 30, 1789.\n"
     ]
    }
   ],
   "source": [
    "#CODE HERE\n",
    "decoded_text = bpe.decode(bpe_encoding.ids)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn_a90UVkv1M"
   },
   "source": [
    "##Explain briefly the changes you have made in the given Tasks in Today's activity (approximately 200 words)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Activity-3, I have made several changes in the given tasks and tried to provide accurate solutions.\n",
    "\n",
    "When it comes to Q-1, The Task gievn was to modify the regular expression and match only digits from a text. I have given a pattern \\d+ instead of [\\w]+ where it ensures that only sequence of digots are tokenized and matched.\n",
    "\n",
    "In the same way in Block-1, for Q-2 The remove_punc to remove punctuation from the text using a regular expression, and remove_stopwords to remove stopwords from the text using a list comprehension were used. These functions were used for  preprocessing the text and for further analysis.\n",
    "\n",
    "In the Q-3, The Task was on Lemmatization on the cleaned text using the NLTK's lemmatizer. Here I have used the 'wordNetLemmatizer' from the 'nltk.stem' module. This iterates over each token and applies lemmatization using the 'lemmatize' method where all tokens are nouns using the POS tag as 'n'/\n",
    "\n",
    "\n",
    "In the Q-4, the given Task was to decode the encoding ID's generated from the given snippet. I have used the decode() method of the \"ByteLevelBPETokenizer' to retrieve original text string. This makes the encoding IDs as input and returns the decoded text.\n",
    "\n",
    "Concluded the changes I have made, These were accurate and functional solutions to the given tasks, I have utilized appropriate libraries and methods for each specific requirement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
